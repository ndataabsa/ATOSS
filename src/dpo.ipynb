{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c35e264f-40da-4b1e-88c6-468c9aeb6337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pickle\n",
    "from functools import partial\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass, field\n",
    "import json, wandb\n",
    "\n",
    "from typing import Dict, Optional\n",
    "from datasets import Dataset, load_dataset, DatasetDict\n",
    "\n",
    "import wandb\n",
    "from peft import LoraConfig\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import seed_everything\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, TrainingArguments\n",
    "from transformers import get_linear_schedule_with_warmup, AutoModelForSeq2SeqLM, AutoTokenizer, EarlyStoppingCallback\n",
    "\n",
    "from trl import DPOTrainer\n",
    "from tqdm import tqdm\n",
    "\n",
    "from atoss.data_utils import *\n",
    "from atoss.eval_utils import *\n",
    "from atoss.process import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74b00be6-a2b0-43cf-9f3e-fa4a53ef6139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method: dpo\n",
      "data path: /home/elicer/ATOSS/data/dpo\n",
      "output path: /home/elicer/ATOSS/outputs\n",
      "load model path: /home/elicer/ATOSS/outputs/sft/rest15_top_12\n",
      "save model path: /home/elicer/ATOSS/outputs/dpo/rest15_top_12\n",
      "rest15_top_12\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        # dataset parameters\n",
    "        self.path = '/home/elicer/ATOSS'\n",
    "        self.method = 'dpo' # task\n",
    "        self.load_model = 'sft/rest15_top_12' # 읽어올 모델 폴더\n",
    "        self.save_model = 'dpo/rest15_top_12' # 저장할 모델 폴더\n",
    "        self.model = 'rest15_top_12' # result.txt애 찍힐 모델 이름\n",
    "        self.task = 'acos' # MVP에서 평가할 데이터 셑\n",
    "        self.dataset = 'rest16' # MVP에서 평가할 데이터 셑\n",
    "        self.train = 'rest15' # reward 이름(데이터 셑이랑 같으면 성능 높음)\n",
    "        self.dev = 'rest16' # dpo에서 eval을 하지 않음(현재는 안씀)\n",
    "        self.eval_data_split = 'test' # test or dev\n",
    "        self.data_path = f'{self.path}/data/{self.method}'\n",
    "        self.ctrl_token = \"post\"\n",
    "        self.data_ratio = 1.0\n",
    "        self.model_name_or_path = 't5-base' # used base model\n",
    "        self.load_ckpt_name = None # 사전 훈련된 모델의 체크포인트 파일로드 \n",
    "        self.beam_size = 1\n",
    "        self.constrained_decode = False\n",
    "        self.lowercase = True\n",
    "        self.load_ckpt_name = None # 사전 훈련된 모델의 체크포인트 파일로드 \n",
    "        self.load_path_cache = False\n",
    "        self.max_seq_length = 512 # 입력 시퀀스 최대 길이\n",
    "        self.eval_batch_size = 16\n",
    "        self.do_train = True # train or not\n",
    "        self.do_inference = True # inference or not\n",
    "\n",
    "        # training parameters\n",
    "        self.beta = 0.1\n",
    "        self.learning_rate = 1e-4\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.max_length = 512\n",
    "        self.max_prompt_length = 512\n",
    "        self.max_target_length = 512\n",
    "        self.label_pad_token_id = -100\n",
    "        self.num_train_epochs = 1\n",
    "        self.batch_size = 16\n",
    "        self.max_steps = -1\n",
    "        \n",
    "        # lora parameters\n",
    "        self.use_peft = False\n",
    "        self.peft_lora_r = 64\n",
    "        self.peft_lora_alpha = 16\n",
    "        \n",
    "        # instrumentation\n",
    "        self.sanity_check = False\n",
    "        self.report_to = 'wandb'  # 결과와 로그를 보고할 통합 목록\n",
    "        \n",
    "        # debug argument for distributed training\n",
    "        self.ignore_bias_buffers = False\n",
    "        self.gradient_checkpointing = False\n",
    "        self.gradient_checkpointing_kwargs = None\n",
    "        \n",
    "        # wandb parameters\n",
    "        self.project_name = \"huggingface\"\n",
    "\n",
    "def init_args():\n",
    "    args = Args()\n",
    "\n",
    "    args.output_dir =  f'{args.path}/outputs'\n",
    "\n",
    "    # set up output dir which looks like './outputs/rest15/'\n",
    "    if not os.path.exists(f'{args.path}/outputs'):\n",
    "        os.mkdir(f'{args.path}/outputs')\n",
    "\n",
    "    if not os.path.exists(args.output_dir):\n",
    "        #os.mkdir(args.output_dir)\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "    return args\n",
    "\n",
    "# Args 인스턴스 생성\n",
    "args = init_args()\n",
    "\n",
    "print('method:', args.method)\n",
    "print('data path:', args.data_path)\n",
    "print('output path:', args.output_dir)\n",
    "print('load model path:', os.path.join(args.output_dir, args.load_model))\n",
    "print('save model path:', os.path.join(args.output_dir, args.save_model))\n",
    "print(args.model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0034cc1b-07f1-4b93-8d0d-b0602530d24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5FineTuner(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Fine tune a pre-trained T5 model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, tfm_model, tokenizer):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['tfm_model'])\n",
    "        self.config = config\n",
    "        self.model = tfm_model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids,\n",
    "                attention_mask=None,\n",
    "                decoder_input_ids=None,\n",
    "                decoder_attention_mask=None,\n",
    "                labels=None):\n",
    "        return self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "    def _step(self, batch):\n",
    "        lm_labels = batch[\"target_ids\"]\n",
    "        lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        outputs = self(input_ids=batch[\"source_ids\"],\n",
    "                       attention_mask=batch[\"source_mask\"],\n",
    "                       labels=lm_labels,\n",
    "                       decoder_attention_mask=batch['target_mask'])\n",
    "\n",
    "        loss = outputs[0]\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def evaluate(self, batch, stage=None):\n",
    "        # get f1\n",
    "        outs = self.model.generate(input_ids=batch['source_ids'],\n",
    "                                   attention_mask=batch['source_mask'],\n",
    "                                   max_length=self.config.max_seq_length,\n",
    "                                   return_dict_in_generate=True,\n",
    "                                   output_scores=True,\n",
    "                                   num_beams=1)\n",
    "\n",
    "        dec = [\n",
    "            self.tokenizer.decode(ids, skip_special_tokens=True)\n",
    "            for ids in outs.sequences\n",
    "        ]\n",
    "        target = [\n",
    "            self.tokenizer.decode(ids, skip_special_tokens=True)\n",
    "            for ids in batch[\"target_ids\"]\n",
    "        ]\n",
    "        scores, _, _ = compute_scores(dec, target, verbose=False)\n",
    "        f1 = torch.tensor(scores['f1'], dtype=torch.float64)\n",
    "\n",
    "        # get loss\n",
    "        loss = self._step(batch)\n",
    "\n",
    "        if stage:\n",
    "            self.log(f\"{stage}_loss\",\n",
    "                     loss,\n",
    "                     prog_bar=True,\n",
    "                     on_step=False,\n",
    "                     on_epoch=True)\n",
    "            self.log(f\"{stage}_f1\",\n",
    "                     f1,\n",
    "                     prog_bar=True,\n",
    "                     on_step=False,\n",
    "                     on_epoch=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\" Prepare optimizer and schedule (linear warmup and decay) \"\"\"\n",
    "        model = self.model\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in model.named_parameters()\n",
    "                    if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\":\n",
    "                self.config.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in model.named_parameters()\n",
    "                    if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\":\n",
    "                0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                          lr=self.config.learning_rate,\n",
    "                          eps=self.config.adam_epsilon)\n",
    "        scheduler = {\n",
    "            \"scheduler\":\n",
    "            get_linear_schedule_with_warmup(optimizer,\n",
    "                                            **self.config.lr_scheduler_init),\n",
    "            \"interval\":\n",
    "            \"step\",\n",
    "        }\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        print(\"load training data.\")\n",
    "        train_dataset = ABSADataset(args=args,\n",
    "                                    tokenizer=tokenizer,\n",
    "                                    task_name=args.task,\n",
    "                                    data_type=args.train,\n",
    "                                    max_len=args.max_seq_length)\n",
    "        dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.train_batch_size,\n",
    "            drop_last=True\n",
    "            if args.data_ratio > 0.3 else False, # don't drop on few-shot\n",
    "            shuffle=True,\n",
    "            num_workers=2)\n",
    "\n",
    "        return dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataset = ABSADataset(args=args,\n",
    "                                    tokenizer=tokenizer,\n",
    "                                    task_name=args.task,\n",
    "                                    data_type=args.dev,\n",
    "                                    max_len=args.max_seq_length)\n",
    "        return DataLoader(val_dataset,\n",
    "                          batch_size=self.config.eval_batch_size,\n",
    "                          num_workers=2)\n",
    "\n",
    "    @staticmethod\n",
    "    def rindex(_list, _value):\n",
    "        return len(_list) - _list[::-1].index(_value) - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12255f70-984f-4e12-b093-25cc1111a33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data load : total num =  Dataset({\n",
      "    features: ['chosen', 'rejected', 'prompt'],\n",
      "    num_rows: 834\n",
      "})\n",
      "dev: data load : total num =  Dataset({\n",
      "    features: ['chosen', 'rejected', 'prompt'],\n",
      "    num_rows: 606\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5122de8877f404da51d5ace0510ed1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/834 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6b119b08f24aad8fdeaf7b398e4f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/anaconda3/envs/atoss/lib/python3.8/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53/53 00:18, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.723800</td>\n",
       "      <td>0.136239</td>\n",
       "      <td>0.094817</td>\n",
       "      <td>-3.549674</td>\n",
       "      <td>0.945254</td>\n",
       "      <td>3.644491</td>\n",
       "      <td>-49.901299</td>\n",
       "      <td>-10.408525</td>\n",
       "      <td>-34.307175</td>\n",
       "      <td>-35.319221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish training and saving the model!\n"
     ]
    }
   ],
   "source": [
    "# do train\n",
    "if args.do_train:\n",
    "\n",
    "    file_path1 = f'{args.data_path}/{args.train}.txt'  # Replace with your file path\n",
    "    file_path2 = f'{args.data_path}/{args.dev}.txt'  # Replace with your file path\n",
    "    dataset_dict = create_dataset(file_path1, file_path2)\n",
    "    train_dataset = dataset_dict['train'] #get_hh(\"train\", sanity_check=args.sanity_check)\n",
    "    eval_dataset = dataset_dict['dev'] #get_hh(\"train\", sanity_check=args.sanity_check)\n",
    "    \n",
    "    print('train data load : total num = ', train_dataset)\n",
    "    print('dev: data load : total num = ', eval_dataset)\n",
    "    \n",
    "    \n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(os.path.join(args.output_dir, args.load_model))\n",
    "    \n",
    "    if args.ignore_bias_buffers:\n",
    "        model._ddp_params_and_buffers_to_ignore = [\n",
    "            name for name, buffer in model.named_buffers() if buffer.dtype == torch.bool\n",
    "        ]\n",
    "    model_ref = AutoModelForSeq2SeqLM.from_pretrained(os.path.join(args.output_dir, args.load_model))\n",
    "    tokenizer = AutoTokenizer.from_pretrained(os.path.join(args.output_dir, args.load_model), lecacy=False, use_fast=False)\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        per_device_train_batch_size=args.batch_size,\n",
    "        per_device_eval_batch_size=args.eval_batch_size,\n",
    "        load_best_model_at_end = True,\n",
    "        max_steps=args.max_steps,\n",
    "        remove_unused_columns=False,\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "        learning_rate=args.learning_rate,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_first_step=True,\n",
    "        logging_steps=1000,  # match results in blog post\n",
    "        eval_steps=500,\n",
    "        num_train_epochs=args.num_train_epochs,\n",
    "        output_dir=os.path.join(args.output_dir, args.save_model),\n",
    "        optim=\"adamw_hf\",\n",
    "        adam_epsilon=1e-8,\n",
    "        warmup_steps=150,\n",
    "        report_to=args.report_to,\n",
    "        bf16=True,\n",
    "        gradient_checkpointing=args.gradient_checkpointing,\n",
    "    )\n",
    "    if args.use_peft:\n",
    "        peft_config = LoraConfig(\n",
    "            r=args.peft_lora_r,\n",
    "            lora_alpha=args.peft_lora_alpha,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "    else:\n",
    "        peft_config = None\n",
    "\n",
    "    dpo_trainer = DPOTrainer(\n",
    "        model,\n",
    "        model_ref,\n",
    "        args=training_args,\n",
    "        beta=args.beta,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=args.max_length,\n",
    "        max_target_length=args.max_target_length,\n",
    "        max_prompt_length=args.max_prompt_length,\n",
    "        generate_during_eval=True,\n",
    "        peft_config=peft_config,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "    )\n",
    "    \n",
    "    dpo_trainer.train()\n",
    "\n",
    "    dpo_trainer.save_model(os.path.join(args.output_dir, args.save_model))\n",
    "    tokenizer.save_pretrained(os.path.join(args.output_dir, args.save_model))\n",
    "    print(\"Finish training and saving the model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ec91393-83d5-498a-ab56-19f776ee05cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****** Conduct inference on trained checkpoint ******\n",
      " model: rest15_top_12, beam: 1, constrained: False\n",
      "\n",
      "Total examples = 583\n",
      "Total examples = 583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/37 [00:00<?, ?it/s]/home/elicer/anaconda3/envs/atoss/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:433: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 37/37 [01:12<00:00,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred labels count Counter({1: 583})\n",
      "gold  yum!\n",
      "pred  yum!\n",
      "\n",
      "gold  serves really good sushi.\n",
      "pred  serves really good sushi.\n",
      "\n",
      "gold  not the biggest portions but adequate.\n",
      "pred  not the biggest portions but adequate.\n",
      "\n",
      "gold  green tea creme brulee is a must!\n",
      "pred  green tea creme brulee is a must!\n",
      "\n",
      "gold  it has great sushi and even better service.\n",
      "pred  it has great sushi and even better service.\n",
      "\n",
      "gold  the entire staff was extremely accomodating and tended to my every need.\n",
      "pred  the entire staff was extremely accomodating and tended to my every need.\n",
      "\n",
      "gold  i've been to this restaurant over a dozen times with no complaints to date.\n",
      "pred  i've been to this restaurant over a dozen times with no complaints to date.\n",
      "\n",
      "gold  the owner is belligerent to guests that have a complaint.\n",
      "pred  the owner is belligerent to guests that have a complaint.\n",
      "\n",
      "gold  good food!\n",
      "pred  good food!\n",
      "\n",
      "gold  this is a great place to get a delicious meal.\n",
      "pred  this is a great place to get a delicious meal.\n",
      "\n",
      "number of gold spans: 583, predicted spans: 583, hit: 468\n",
      "model: rest15_top_12 data: test precision: 80.27 recall: 80.27 F1 = 80.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# do inference\n",
    "if args.do_inference:\n",
    "        \n",
    "    tfm_model = T5ForConditionalGeneration.from_pretrained(os.path.join(args.output_dir, args.save_model))\n",
    "    tokenizer = AutoTokenizer.from_pretrained(os.path.join(args.output_dir, args.save_model), lecacy=False, use_fast=False)\n",
    "    model = T5FineTuner(args, tfm_model, tokenizer)\n",
    "    # inference\n",
    "    print(\"\\n****** Conduct inference on trained checkpoint ******\")\n",
    "    \n",
    "    args.output_dir = f'{args.output_dir}/{args.method}'\n",
    "    \n",
    "    if args.load_ckpt_name:\n",
    "        ckpt_path = os.path.join(args.output_dir, args.load_ckpt_name)\n",
    "        print(\"Loading ckpt:\", ckpt_path)\n",
    "        checkpoint = torch.load(ckpt_path)\n",
    "        model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    \n",
    "    log_file_path = os.path.join(args.output_dir, \"result.txt\")\n",
    "    \n",
    "    # compute the performance scores\n",
    "    with open(log_file_path, \"a+\") as f:\n",
    "        config_str = f\" model: {args.model}, beam: {args.beam_size}, constrained: {args.constrained_decode}\\n\"\n",
    "        print(config_str)\n",
    "        f.write(config_str)\n",
    "        scores = evaluate(args,\n",
    "                          model,\n",
    "                          args.task,\n",
    "                          data_type=args.eval_data_split)\n",
    "    \n",
    "        exp_results = \"model: {} data: {} precision: {:.2f} recall: {:.2f} F1 = {:.2f}\".format(\n",
    "            args.model, args.eval_data_split, scores['precision'], scores['recall'], scores['f1'])\n",
    "        print(exp_results)\n",
    "        f.write(exp_results + \"\\n\")\n",
    "        f.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67d4d020-5141-48b6-a312-2a5b8d372de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yum!'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = os.path.join(\n",
    "        args.output_dir, \"rst_{}{}_{}{}_{}_{}{}beam{}.pickle\".format(\n",
    "            args.method,\n",
    "            args.model,\n",
    "            args.task,\n",
    "            args.dataset,\n",
    "            args.eval_data_split,\n",
    "            \"best_\" if args.load_ckpt_name else \"\",\n",
    "            \"cd_\" if args.constrained_decode else \"\",\n",
    "            args.beam_size))\n",
    "with open(file_path, 'rb') as f:\n",
    "    loaded_object = pd.read_pickle(f)\n",
    "loaded_object[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c4bd15a-157d-40a6-9399-e229330147aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read. Total count:  583\n",
      "Input count: 583\n",
      "Expanded target count: 583\n",
      "Merged data count: 583\n",
      "Data return. Total count: 583\n",
      "/home/elicer/ABSA/data/acos/rest16/dpo_rest15_top_12_test.txt\n"
     ]
    }
   ],
   "source": [
    "file = f'/home/elicer/ABSA/data/{args.task}/{args.dataset}/test.txt'\n",
    "_, targets = split_sharp(file)\n",
    "sft = merge_sharp_n(loaded_object[0],targets)\n",
    "file_name = f'/home/elicer/ABSA/data/{args.task}/{args.dataset}/{args.method}_{args.model}_test.txt'\n",
    "print(file_name)\n",
    "with open(file_name, 'w', encoding='UTF-8') as file:\n",
    "    for line in sft:\n",
    "        file.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e4091a72-8c14-439b-bab5-3be16160bd4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zero1440'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1a1857-63f3-4760-af73-37717da22129",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atoss",
   "language": "python",
   "name": "atoss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
